{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25aeb9c5-b709-45a7-a5a7-53d706b47178",
   "metadata": {},
   "source": [
    "# MAE 598: Design Optimization - Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24039b-678f-440d-ae52-c9f8623285a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "\n",
    "$$ {f(x_1, x_2) = 2x^2_1 - 4x_1x_2 + 1.5x^2_2 + x_2} $$\n",
    "\n",
    "\n",
    "Gradient of $ f(x_1, x_2) $ is:\n",
    "\n",
    "$$ {\n",
    "g(x_1, x_2) = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}\\\\\n",
    "\\frac{\\partial f}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    " = \\begin{bmatrix}\n",
    "4x_1 - 4x_2\\\\\n",
    "-4x_1 + 3x_2 + 1\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "To find the saddle point, $ g(x) = 0 $, solving the the two linear eqauations simultaneously, we get:\n",
    "\n",
    "$$ x_1 = 1, x_2 = 1 $$\n",
    "\n",
    "\n",
    "Hessian of $ f(x_1, x_2) $ is:\n",
    "\n",
    "$$ {\n",
    "H(x_1, x_2) = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x^2_1} & \\frac{\\partial^2 f}{\\partial x_1x_2}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_1x_2} & \\frac{\\partial^2 f}{\\partial x^2_2}\n",
    "\\end{bmatrix}\n",
    " = \\begin{bmatrix}\n",
    "4 & -4\\\\\n",
    "-4 & 3\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "\n",
    "$$ {\n",
    "\\left|\n",
    "\\begin{array}{ccc}\n",
    "H(x_1, x_2) - \\lambda I\n",
    "\\end{array}\n",
    "\\right| = 0\n",
    "} $$\n",
    "\n",
    "$$\n",
    "\\left|\n",
    "\\begin{array}{ccc}\n",
    "4 - \\lambda & -4\\\\\n",
    "-4 & 3 - \\lambda\n",
    "\\end{array}\n",
    "\\right| = (4 - \\lambda) (3 - \\lambda) - 16 = \\lambda^2 - 7\\lambda - 4 = 0\n",
    "$$\n",
    "\n",
    "Solving the characteristic equation, the two Eigenvalues are: $ \\lambda_1 = 7.53, \\lambda_2 = -0.53. $\n",
    "Since the Eigenvalues are both positive and negetive, Hessian is indefinite.\n",
    "\n",
    "\n",
    "\n",
    "Taylor's Second order approximation at the saddle point $x_0 = (1,1)$:\n",
    "\n",
    "$$ {f(x_1, x_2) = f(x_0) +  \\left.g(x)^T\\right|_{x_0} . (x - x_0) + \\frac{1}{2} (x - x_0)^T . \\left.H(x)\\right|_{x_0} . (x - x_0)} $$\n",
    "\n",
    "Evaluating $ f(x_1, x_2) $ and $ g(x_1, x_2) $ at $x_0 = (1,1)$\n",
    "\n",
    "$$ {f(1, 1) = 2 (1)^2 - 4 (1 . 1) + 1.5 (1)^2 + 1 = 0.5} $$\n",
    "\n",
    "$$ {\n",
    "g(1, 1) = \\begin{bmatrix}\n",
    "4 (1) - 4 (1)\\\\\n",
    "-4 (1) + 3 (1) + 1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "Also $\\partial x_i = x_i - 1 $ for $ i = 1, 2 $\n",
    "We can write $ (x - x_0) $ as:\n",
    "\n",
    "$$ {\n",
    "(x - x_0) = \\begin{bmatrix}\n",
    "x_1 - 1\\\\\n",
    "x_2 - 1\n",
    "\\end{bmatrix}\n",
    " = \\begin{bmatrix}\n",
    "\\partial x_1\\\\\n",
    "\\partial x_2\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "\n",
    "Substituting these results back into the approximation function:\n",
    "\n",
    "$$ {f(x_1, x_2) = 0.5 +  \n",
    "\\begin{bmatrix} 0 & 0 \\end{bmatrix} . \n",
    "\\begin{bmatrix}\n",
    "\\partial x_1\\\\\n",
    "\\partial x_2\n",
    "\\end{bmatrix} + \n",
    "\\frac{1}{2} \\begin{bmatrix}\n",
    "\\partial x_1 & \\partial x_2\n",
    "\\end{bmatrix} . \n",
    "\\begin{bmatrix}\n",
    "4 & -4\\\\\n",
    "-4 & 3\n",
    "\\end{bmatrix} . \n",
    "\\begin{bmatrix}\n",
    "\\partial x_1\\\\\n",
    "\\partial x_2\n",
    "\\end{bmatrix}} $$\n",
    "\n",
    "$$ {f(x_1, x_2) = 0.5 +  \n",
    "0 + \n",
    "\\frac{1}{2} \\begin{bmatrix}\n",
    "\\partial x_1 & \\partial x_2\n",
    "\\end{bmatrix} . \n",
    "\\begin{bmatrix}\n",
    "4\\partial x_1 - 4\\partial x_2\\\\\n",
    "-4\\partial x_1 + 3\\partial x_2\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \\frac{1}{2}\n",
    "(\n",
    "4\\partial^2 x_1 - 4\\partial x_1\\partial x_2 - 4\\partial x_1\\partial x_2 + 3\\partial^2 x_2\n",
    ")\n",
    "} $$\n",
    "\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \\frac{1}{2}\n",
    "(\n",
    "4\\partial^2 x_1 - 8\\partial x_1\\partial x_2 + 3\\partial^2 x_2\n",
    ")\n",
    "} $$\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \\frac{1}{2}\n",
    "(\n",
    "4\\partial^2 x_1 - 8\\partial x_1\\partial x_2 + 3\\partial^2 x_2\n",
    ")\n",
    "} $$\n",
    "\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \\frac{1}{2}\n",
    "(2\\partial x_1 - \\partial x_2) (2\\partial x_1 - 3\\partial x_2)\n",
    "} $$\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \n",
    "(\\partial x_1 - \\frac{\\partial x_2}{2}) (\\partial x_1 - \\frac{3\\partial x_2}{2})\n",
    "} $$\n",
    "\n",
    "\n",
    "The constants $ a, b, c, d $ are $ 1, -1/2, 1, -3/2 $ respectively.\n",
    "\n",
    "The direction of the downslope is in the negative gradient direction, so we get\n",
    "\n",
    "$$ {f(x_1, x_2) - 0.5 =   \n",
    "(\\partial x_1 - \\frac{\\partial x_2}{2}) . (\\partial x_1 - \\frac{3\\partial x_2}{2}) < 0\n",
    "} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dec9aa-af03-4f6e-96e0-8b7ba9661831",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "### Part (a)\n",
    "\n",
    "\n",
    "\n",
    "### Part (b)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8517c5c7-9d03-4a09-b023-d6bb45c4c210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent\n",
      "\n",
      "Solution #1\n",
      "\n",
      "Iteration: 1;\t\talpha: 0.03125;\t\tx: [[0.25   0.4375]]\n",
      "Iteration: 2;\t\talpha: 0.03125;\t\tx: [[0.2578125 0.5078125]]\n",
      "Iteration: 3;\t\talpha: 0.25;\t\tx: [[0.08984375 0.6953125 ]]\n",
      "Iteration: 4;\t\talpha: 0.03125;\t\tx: [[0.05102539 0.66455078]]\n",
      "Iteration: 5;\t\talpha: 0.125;\t\tx: [[-0.00958252  0.67663574]]\n",
      "Iteration: 6;\t\talpha: 0.03125;\t\tx: [[-0.01032639  0.69483185]]\n",
      "Iteration: 7;\t\talpha: 0.25;\t\tx: [[-0.06900597  0.75165176]]\n",
      "Iteration: 8;\t\talpha: 0.03125;\t\tx: [[-0.07931101  0.74524665]]\n",
      "Iteration: 9;\t\talpha: 0.125;\t\tx: [[-0.09804222  0.75109655]]\n",
      "Iteration: 10;\t\talpha: 0.0625;\t\tx: [[-0.10008824  0.76075753]]\n",
      "Iteration: 11;\t\talpha: 0.0625;\t\tx: [[-0.10810124  0.7598768 ]]\n",
      "Iteration: 12;\t\talpha: 0.0625;\t\tx: [[-0.11044556  0.76610673]]\n",
      "Iteration: 13;\t\talpha: 0.0625;\t\tx: [[-0.11599713  0.76630749]]\n",
      "Iteration: 14;\t\talpha: 0.125;\t\tx: [[-0.12046195  0.77453446]]\n",
      "Iteration: 15;\t\talpha: 0.03125;\t\tx: [[-0.12326802  0.77312366]]\n",
      "Iteration: 16;\t\talpha: 0.25;\t\tx: [[-0.13446894  0.77730943]]\n",
      "Iteration: 17;\t\talpha: 0.03125;\t\tx: [[-0.13393843  0.77941689]]\n",
      "Iteration: 18;\t\talpha: 0.125;\t\tx: [[-0.13564072  0.78178232]]\n",
      "Iteration: 19;\t\talpha: 0.03125;\t\tx: [[-0.13642137  0.78153364]]\n",
      "Iteration: 20;\t\talpha: 0.25;\t\tx: [[-0.13996887  0.78312954]]\n",
      "Iteration: 21;\t\talpha: 0.03125;\t\tx: [[-0.13990218  0.7836619 ]]\n",
      "Iteration: 22;\t\talpha: 0.25;\t\tx: [[-0.14113245  0.78505891]]\n",
      "Iteration: 23;\t\talpha: 0.03125;\t\tx: [[-0.14142565  0.78482176]]\n",
      "Iteration: 24;\t\talpha: 0.125;\t\tx: [[-0.14187623  0.78490583]]\n",
      "Iteration: 25;\t\talpha: 0.03125;\t\tx: [[-0.14187959  0.78504327]]\n",
      "Iteration: 26;\t\talpha: 0.25;\t\tx: [[-0.14231043  0.78546569]]\n",
      "Iteration: 27;\t\talpha: 0.03125;\t\tx: [[-0.14238805  0.78541604]]\n",
      "Iteration: 28;\t\talpha: 0.125;\t\tx: [[-0.14252705  0.78545801]]\n",
      "Iteration: 29;\t\talpha: 0.0625;\t\tx: [[-0.14254115  0.78553079]]\n",
      "Iteration: 30;\t\talpha: 0.0625;\t\tx: [[-0.14260102  0.78552317]]\n",
      "Iteration: 31;\t\talpha: 0.0625;\t\tx: [[-0.14261776  0.78556997]]\n",
      "Iteration: 32;\t\talpha: 0.0625;\t\tx: [[-0.14265914  0.78557083]]\n",
      "Iteration: 33;\t\talpha: 0.125;\t\tx: [[-0.14269145  0.78563247]]\n",
      "Iteration: 34;\t\talpha: 0.03125;\t\tx: [[-0.14271255  0.78562147]]\n",
      "Iteration: 35;\t\talpha: 0.25;\t\tx: [[-0.14279559  0.78565177]]\n",
      "Iteration: 36;\t\talpha: 0.03125;\t\tx: [[-0.14279138  0.78566776]]\n",
      "Iteration: 37;\t\talpha: 0.125;\t\tx: [[-0.14280379  0.78568543]]\n",
      "Iteration: 38;\t\talpha: 0.03125;\t\tx: [[-0.14280964  0.78568346]]\n",
      "Iteration: 39;\t\talpha: 0.25;\t\tx: [[-0.14283591  0.7856951 ]]\n",
      "Iteration: 40;\t\talpha: 0.03125;\t\tx: [[-0.14283535  0.78569913]]\n",
      "\n",
      "Solution #2\n",
      "\n",
      "Iteration: 1;\t\talpha: 0.03125;\t\tx: [[31.5     0.4375]]\n",
      "Iteration: 2;\t\talpha: 0.03125;\t\tx: [[ 21.7421875 -11.2109375]]\n",
      "Iteration: 3;\t\talpha: 0.03125;\t\tx: [[ 19.40185547 -11.91992188]]\n",
      "Iteration: 4;\t\talpha: 0.25;\t\tx: [[ 8.65698242 -7.02587891]]\n",
      "Iteration: 5;\t\talpha: 0.03125;\t\tx: [[ 8.83638  -5.443573]]\n",
      "Iteration: 6;\t\talpha: 0.25;\t\tx: [[ 5.07614899 -1.23484802]]\n",
      "Iteration: 7;\t\talpha: 0.03125;\t\tx: [[ 4.20292044 -1.92912388]]\n",
      "Iteration: 8;\t\talpha: 0.125;\t\tx: [[ 2.84295571 -1.66069484]]\n",
      "Iteration: 9;\t\talpha: 0.03125;\t\tx: [[ 2.82729261 -1.25136895]]\n",
      "Iteration: 10;\t\talpha: 0.25;\t\tx: [[1.51316794 0.02359798]]\n",
      "Iteration: 11;\t\talpha: 0.03125;\t\tx: [[ 1.28145372 -0.12108874]]\n",
      "Iteration: 12;\t\talpha: 0.125;\t\tx: [[0.86126968 0.00945253]]\n",
      "Iteration: 13;\t\talpha: 0.0625;\t\tx: [[0.81588673 0.22668461]]\n",
      "Iteration: 14;\t\talpha: 0.0625;\t\tx: [[0.63594407 0.2064138 ]]\n",
      "Iteration: 15;\t\talpha: 0.0625;\t\tx: [[0.58366868 0.3464385 ]]\n",
      "Iteration: 16;\t\talpha: 0.0625;\t\tx: [[0.45904688 0.35063887]]\n",
      "Iteration: 17;\t\talpha: 0.125;\t\tx: [[0.35927998 0.53547138]]\n",
      "Iteration: 18;\t\talpha: 0.03125;\t\tx: [[0.29620322 0.50357178]]\n",
      "Iteration: 19;\t\talpha: 0.25;\t\tx: [[0.04497985 0.59710324]]\n",
      "Iteration: 20;\t\talpha: 0.03125;\t\tx: [[0.05700993 0.64454627]]\n",
      "Iteration: 21;\t\talpha: 0.125;\t\tx: [[0.01892811 0.6976657 ]]\n",
      "Iteration: 22;\t\talpha: 0.03125;\t\tx: [[0.00138844 0.6920266 ]]\n",
      "Iteration: 23;\t\talpha: 0.25;\t\tx: [[-0.07816244  0.7277283 ]]\n",
      "Iteration: 24;\t\talpha: 0.03125;\t\tx: [[-0.07663479  0.73970903]]\n",
      "Iteration: 25;\t\talpha: 0.25;\t\tx: [[-0.1041749   0.77106826]]\n",
      "Iteration: 26;\t\talpha: 0.03125;\t\tx: [[-0.11077084  0.76571619]]\n",
      "Iteration: 27;\t\talpha: 0.125;\t\tx: [[-0.12088157  0.76758198]]\n",
      "Iteration: 28;\t\talpha: 0.03125;\t\tx: [[-0.12094932  0.77067383]]\n",
      "Iteration: 29;\t\talpha: 0.25;\t\tx: [[-0.13059751  0.78015264]]\n",
      "Iteration: 30;\t\talpha: 0.03125;\t\tx: [[-0.13234303  0.77903131]]\n",
      "Iteration: 31;\t\talpha: 0.125;\t\tx: [[-0.1354612   0.77996758]]\n",
      "Iteration: 32;\t\talpha: 0.0625;\t\tx: [[-0.13577364  0.78160401]]\n",
      "Iteration: 33;\t\talpha: 0.0625;\t\tx: [[-0.13711812  0.78142923]]\n",
      "Iteration: 34;\t\talpha: 0.0625;\t\tx: [[-0.13749122  0.78248128]]\n",
      "Iteration: 35;\t\talpha: 0.0625;\t\tx: [[-0.13842017  0.78249809]]\n",
      "Iteration: 36;\t\talpha: 0.0625;\t\tx: [[-0.13878113  0.7831906 ]]\n",
      "Iteration: 37;\t\talpha: 0.125;\t\tx: [[-0.14009062  0.78338579]]\n",
      "Iteration: 38;\t\talpha: 0.03125;\t\tx: [[-0.14008197  0.78380366]]\n",
      "Iteration: 39;\t\talpha: 0.25;\t\tx: [[-0.141288   0.7850313]]\n",
      "Iteration: 40;\t\talpha: 0.03125;\t\tx: [[-0.14152224  0.78486974]]\n",
      "Iteration: 41;\t\talpha: 0.125;\t\tx: [[-0.14192405  0.78497875]]\n",
      "Iteration: 42;\t\talpha: 0.03125;\t\tx: [[-0.14193982  0.78508855]]\n",
      "Iteration: 43;\t\talpha: 0.25;\t\tx: [[-0.14235593  0.78546525]]\n",
      "Iteration: 44;\t\talpha: 0.03125;\t\tx: [[-0.14241917  0.78543294]]\n",
      "Iteration: 45;\t\talpha: 0.25;\t\tx: [[-0.14267007  0.78552574]]\n",
      "Iteration: 46;\t\talpha: 0.03125;\t\tx: [[-0.14265783  0.78557343]]\n",
      "Iteration: 47;\t\talpha: 0.125;\t\tx: [[-0.14269569  0.78562659]]\n",
      "Iteration: 48;\t\talpha: 0.03125;\t\tx: [[-0.14271326  0.78562086]]\n",
      "Iteration: 49;\t\talpha: 0.25;\t\tx: [[-0.14279268  0.78565635]]\n",
      "Iteration: 50;\t\talpha: 0.03125;\t\tx: [[-0.1427911   0.78566839]]\n",
      "Iteration: 51;\t\talpha: 0.25;\t\tx: [[-0.14281851  0.78569975]]\n",
      "Iteration: 52;\t\talpha: 0.03125;\t\tx: [[-0.14282513  0.78569435]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grad_desc(x):\n",
    "    # Objective function\n",
    "    f = lambda x: (2 - 2 * x[0] - 3 * x[1])**2 + x[0]**2 + (x[1] - 1)**2\n",
    "    # Gradient of the objective function\n",
    "    grad = lambda x: np.array([[10 * x[0] + 12 * x[1] - 8], [12 * x[0] + 20 * x[1] - 14]])\n",
    "    # Hessian of the objective function\n",
    "    hess = np.array([[10, 12],\n",
    "                     [12, 20]])\n",
    "\n",
    "    # Initialization\n",
    "    t = 0.5\n",
    "    e = np.linalg.norm(grad(x))\n",
    "    tol = 1e-4\n",
    "    itr = 50\n",
    "    xSol = x.T\n",
    "    alphaSol = [1]\n",
    "    \n",
    "    j = 0\n",
    "    while e > tol:\n",
    "\n",
    "        alpha = 1\n",
    "        i = 0\n",
    "        f1 = f(x - alpha * grad(x)[:, :, 0])[0]\n",
    "        phi = f(x)[0] - (t * alpha * (grad(x)[:, :, 0].T @ grad(x)[:, :, 0])[0, 0]) \n",
    "        # Inexact line search\n",
    "        while f1 > phi and i < itr:\n",
    "            alpha = 0.5 * alpha\n",
    "            f1 = f(x - alpha * grad(x)[:, :, 0])[0]\n",
    "            phi = f(x)[0] - (t * alpha * (grad(x)[:, :, 0].T @ grad(x)[:, :, 0])[0, 0])\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        x = x - alpha * grad(x)[:, :, 0]\n",
    "        xSol = np.concatenate((xSol, x.T), axis=0)\n",
    "        alphaSol.append(alpha)\n",
    "        e = np.linalg.norm(grad(x))\n",
    "        j += 1\n",
    "        print(f'Iteration: {j};' + '\\t\\t' + f'alpha: {alpha};' + '\\t\\t'  +  f'x: {x.T}')\n",
    "    return xSol, alphaSol\n",
    "        \n",
    "\n",
    "print(\"Gradient Descent\")\n",
    "# Initial guess 1    \n",
    "x1 = np.array([[0], [0]])\n",
    "print('\\nSolution #1\\n')\n",
    "xSol1, alphaSol1 = grad_desc(x1)\n",
    "\n",
    "\n",
    "# Initial guess 2\n",
    "x2 = np.array([[100], [100]])\n",
    "print('\\nSolution #2\\n')\n",
    "xSol2, alphaSol2 = grad_desc(x2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ded9c84-6128-4e99-b9f3-e23d83caa826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton's Method\n",
      "\n",
      "Solution #1\n",
      "\n",
      "Iteration: 1;\t\tx: [[-0.14285714  0.78571429]]\n",
      "\n",
      "Solution #2\n",
      "\n",
      "Iteration: 1;\t\tx: [[-0.14285714  0.78571429]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def newt(x):\n",
    "    # Objective function\n",
    "    f = lambda x: (2 - 2 * x[0] - 3 * x[1])**2 + x[0]**2 + (x[1] - 1)**2\n",
    "    # Gradient of the objective function\n",
    "    grad = lambda x: np.array([[10 * x[0] + 12 * x[1] - 8], [12 * x[0] + 20 * x[1] - 14]])\n",
    "    # Hessian of the objective function\n",
    "    hess = np.array([[10, 12],\n",
    "                     [12, 20]])\n",
    "    hinv = np.linalg.inv(hess)\n",
    "\n",
    "    # Initialization\n",
    "    x = np.array([[0], [0]])\n",
    "    t = 0.5\n",
    "    e = np.linalg.norm(grad(x))\n",
    "    tol = 1e-3\n",
    "    itr = 100\n",
    "    xSol = x.T\n",
    "    alphaSol = [1]\n",
    "\n",
    "    j = 0\n",
    "    while e > tol:\n",
    "        x = x - hinv @ grad(x)[:, :, 0]\n",
    "        xSol = np.concatenate((xSol, x.T), axis=0)\n",
    "\n",
    "        e = np.linalg.norm(grad(x))\n",
    "        j += 1\n",
    "        print(f'Iteration: {j};' + '\\t\\t' +  f'x: {x.T}')\n",
    "    return xSol\n",
    "\n",
    "\n",
    "print(\"Newton's Method\")\n",
    "# Initial guess 1    \n",
    "x1 = np.array([[0], [0]])\n",
    "print('\\nSolution #1\\n')\n",
    "xSol1, alphaSol1 = newt(x1)\n",
    "\n",
    "\n",
    "# Initial guess 2\n",
    "x2 = np.array([[100], [100]])\n",
    "print('\\nSolution #2\\n')\n",
    "xSol2, alphaSol2 = newt(x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e50434-50bb-41c4-ba03-e619dafe32f7",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Hyperplane in $ \\mathbb{R}^n $ is defined as:\n",
    "\n",
    "$ {a^Tx = c} $ for $ x \\in \\mathbb{R}^n $, where $ a $ is the direction of the normal to the hyperplane and c is some constant.\n",
    "\n",
    "To prove that the hyperplane is a conves state, we consider two point $ x_1 $ and $ x_2 $ on the plane, such that\n",
    "\n",
    "$$ {a^Tx_1 = c} $$\n",
    "$$ {a^Tx_2 = c} $$\n",
    "\n",
    "If this point lies on the hyperplane then the plane is a convex set.\n",
    "$$ \\lambda x_1 + (1 - \\lambda) x_2 \\quad \\forall \\lambda \\in [0, 1] $$\n",
    "\n",
    "Substituting this expression in the $ LHS $ of the hyperplane definition\n",
    "$$ a^T(\\lambda x_1 + (1 - \\lambda) x_2) = a^T \\ \\lambda x_1 + a^T \\ (1 - \\lambda) x_2 = \\lambda \\ a^T x_1 + a^Tx_2 - \\lambda \\ a^Tx_2 = \\lambda \\ c + c - \\lambda \\ c = c = RHS $$\n",
    "\n",
    "Thus we show that the any point on the line joining $ x_1 $ and $ x_2 $ also lies on the plane. Therefore hyperplane is a convex set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859d4a7-f6f0-4263-9262-526a9f4c0cfc",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "The objective function is:\n",
    "\n",
    "$$ \\min_{p} \\quad \\max_{k} \\{h(a^T_k \\ p, I_t)\\} $$\n",
    "\n",
    "Subject to: $ 0 \\le p_i \\le p_{max} $\n",
    "\n",
    "\n",
    "$$\n",
    "I = a^T_kp\n",
    "$$\n",
    "\n",
    "$$\n",
    "h(I, I_t) = \\begin{cases} \n",
    "I_t \\ / \\ I \\quad I \\le I_t \\\\\n",
    "I \\ / \\ I_t \\quad I_t \\le I\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Part (a)\n",
    "\n",
    "To show that the objective function is convex, we start by checking if the function $ h(I, I_t) $ is convex.\n",
    "\n",
    "Gradient of $ h $:\n",
    "\n",
    "$$ \\frac{\\partial h}{\\partial p} =  \\frac{d h}{d I} \\ \\frac{d I}{d p} = h' \\frac{d (a^T_kp)}{d p}  = h'.a$$\n",
    "\n",
    "Hessian of $ h $:\n",
    "\n",
    "$$ \\frac{\\partial^2 h}{\\partial p^2} =  \\frac{d h'}{d I} \\ \\frac{d I}{d p} \\ a^T = h'' \\ a.a^T $$\n",
    "\n",
    "\n",
    "$$\n",
    "h''(I, I_t) = \\begin{cases} \n",
    "2I_t \\ / \\ I^3 \\quad I \\le I_t \\\\\n",
    "1 \\ / \\ I_t \\quad I_t \\le I\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "From the above expression, we can determine that $ h'' \\ge 0 $ since $ I > 0 $.\n",
    "Also, $ a.a^T \\ge 0 $.\n",
    "Therefore, Hessian of $ h $ is positive semi definite. So $ h $ is convex function. \n",
    "\n",
    "Further, the maximum of $ h $ and a scalar $ I_t $ is also a convex function. Therefore the problem is convex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c9474-763d-4f67-8437-e4b8d450643c",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Given function is:\n",
    "\n",
    "$$ c^*(y) = \\max_{x} \\ \\{xy - c(x)\\} $$\n",
    "\n",
    "First check if function $ f(y) = xy - c(x) $ is convex.\n",
    "\n",
    "Gradient of $ f(y) $ is:\n",
    "\n",
    "$$ g(y) = x $$\n",
    "\n",
    "Hessian of $ f(y) $ is:\n",
    "\n",
    "$$ H(y) = 0 $$\n",
    "\n",
    "Since $ f(y) $ is a linear function of $ y $ it follows that it is a convex function. But, maximum function of a convex function is also convex. Hence $ c^*(y) $ is a convex function with respect to $ y $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b163bd-b307-4be3-94be-b98132c55446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
