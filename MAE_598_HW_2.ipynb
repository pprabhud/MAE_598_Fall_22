{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25aeb9c5-b709-45a7-a5a7-53d706b47178",
   "metadata": {},
   "source": [
    "# MAE 598: Design Optimization - Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24039b-678f-440d-ae52-c9f8623285a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "\n",
    "$$ {f(x_1, x_2) = 2x^2_1 - 4x_1x_2 + 1.5x^2_2 + x_2} $$\n",
    "\n",
    "\n",
    "Gradient of $ f(x_1, x_2) $ is:\n",
    "\n",
    "$$ {\n",
    "g(x_1, x_2) = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}\\\\\n",
    "\\frac{\\partial f}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    " = \\begin{bmatrix}\n",
    "4x_1 - 4x_2\\\\\n",
    "-4x_1 + 3x_2 + 1\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "To find the saddle point, $ g(x) = 0 $, solving the the two linear eqauations simultaneously, we get:\n",
    "\n",
    "$$ x_1 = 1, x_2 = 1 $$\n",
    "\n",
    "\n",
    "Hessian of $ f(x_1, x_2) $ is:\n",
    "\n",
    "$$ {\n",
    "H(x_1, x_2) = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x^2_1} & \\frac{\\partial^2 f}{\\partial x_1x_2}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_1x_2} & \\frac{\\partial^2 f}{\\partial x^2_2}\n",
    "\\end{bmatrix}\n",
    " = \\begin{bmatrix}\n",
    "4 & -4\\\\\n",
    "-4 & 3\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "\n",
    "$$ {\n",
    "\\left|\n",
    "\\begin{array}{ccc}\n",
    "H(x_1, x_2) - \\lambda I\n",
    "\\end{array}\n",
    "\\right| = 0\n",
    "} $$\n",
    "\n",
    "$$\n",
    "\\left|\n",
    "\\begin{array}{ccc}\n",
    "4 - \\lambda & -4\\\\\n",
    "-4 & 3 - \\lambda\n",
    "\\end{array}\n",
    "\\right| = (4 - \\lambda) (3 - \\lambda) - 16 = \\lambda^2 - 7\\lambda - 4 = 0\n",
    "$$\n",
    "\n",
    "Solving the characteristic equation, the two Eigenvalues are: $ \\lambda_1 = 7.53, \\lambda_2 = -0.53. $\n",
    "Since the Eigenvalues are both positive and negetive, Hessian is indefinite.\n",
    "\n",
    "\n",
    "\n",
    "Taylor's Second order approximation at the saddle point $x_0 = (1,1)$:\n",
    "\n",
    "$$ {f(x_1, x_2) = f(x_0) +  \\left.g(x)^T\\right|_{x_0} . (x - x_0) + \\frac{1}{2} (x - x_0)^T . \\left.H(x)\\right|_{x_0} . (x - x_0)} $$\n",
    "\n",
    "Evaluating $ f(x_1, x_2) $ and $ g(x_1, x_2) $ at $x_0 = (1,1)$\n",
    "\n",
    "$$ {f(1, 1) = 2 (1)^2 - 4 (1 . 1) + 1.5 (1)^2 + 1 = 0.5} $$\n",
    "\n",
    "$$ {\n",
    "g(1, 1) = \\begin{bmatrix}\n",
    "4 (1) - 4 (1)\\\\\n",
    "-4 (1) + 3 (1) + 1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "Also $\\partial x_i = x_i - 1 $ for $ i = 1, 2 $\n",
    "We can write $ (x - x_0) $ as:\n",
    "\n",
    "$$ {\n",
    "(x - x_0) = \\begin{bmatrix}\n",
    "x_1 - 1\\\\\n",
    "x_2 - 1\n",
    "\\end{bmatrix}\n",
    " = \\begin{bmatrix}\n",
    "\\partial x_1\\\\\n",
    "\\partial x_2\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "\n",
    "Substituting these results back into the approximation function:\n",
    "\n",
    "$$ {f(x_1, x_2) = 0.5 +  \n",
    "\\begin{bmatrix} 0 & 0 \\end{bmatrix} . \n",
    "\\begin{bmatrix}\n",
    "\\partial x_1\\\\\n",
    "\\partial x_2\n",
    "\\end{bmatrix} + \n",
    "\\frac{1}{2} \\begin{bmatrix}\n",
    "\\partial x_1 & \\partial x_2\n",
    "\\end{bmatrix} . \n",
    "\\begin{bmatrix}\n",
    "4 & -4\\\\\n",
    "-4 & 3\n",
    "\\end{bmatrix} . \n",
    "\\begin{bmatrix}\n",
    "\\partial x_1\\\\\n",
    "\\partial x_2\n",
    "\\end{bmatrix}} $$\n",
    "\n",
    "$$ {f(x_1, x_2) = 0.5 +  \n",
    "0 + \n",
    "\\frac{1}{2} \\begin{bmatrix}\n",
    "\\partial x_1 & \\partial x_2\n",
    "\\end{bmatrix} . \n",
    "\\begin{bmatrix}\n",
    "4\\partial x_1 - 4\\partial x_2\\\\\n",
    "-4\\partial x_1 + 3\\partial x_2\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \\frac{1}{2}\n",
    "(\n",
    "4\\partial^2 x_1 - 4\\partial x_1\\partial x_2 - 4\\partial x_1\\partial x_2 + 3\\partial^2 x_2\n",
    ")\n",
    "} $$\n",
    "\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \\frac{1}{2}\n",
    "(\n",
    "4\\partial^2 x_1 - 8\\partial x_1\\partial x_2 + 3\\partial^2 x_2\n",
    ")\n",
    "} $$\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \\frac{1}{2}\n",
    "(\n",
    "4\\partial^2 x_1 - 8\\partial x_1\\partial x_2 + 3\\partial^2 x_2\n",
    ")\n",
    "} $$\n",
    "\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \\frac{1}{2}\n",
    "(2\\partial x_1 - \\partial x_2) (2\\partial x_1 - 3\\partial x_2)\n",
    "} $$\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \n",
    "(\\partial x_1 - \\frac{\\partial x_2}{2}) (\\partial x_1 - \\frac{3\\partial x_2}{2})\n",
    "} $$\n",
    "\n",
    "\n",
    "The constants $ a, b, c, d $ are $ 1, -1/2, 1, -3/2 $ respectively.\n",
    "\n",
    "The direction of the downslope is in the negative gradient direction, so we get\n",
    "\n",
    "$$ {f(x_1, x_2) - 0.5 =   \n",
    "(\\partial x_1 - \\frac{\\partial x_2}{2}) . (\\partial x_1 - \\frac{3\\partial x_2}{2}) < 0\n",
    "} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dec9aa-af03-4f6e-96e0-8b7ba9661831",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e50434-50bb-41c4-ba03-e619dafe32f7",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Hyperplane in $ \\mathbb{R}^n $ is defined as:\n",
    "\n",
    "$ {a^Tx = c} $ for $ x \\in \\mathbb{R}^n $, where $ a $ is the direction of the normal to the hyperplane and c is some constant.\n",
    "\n",
    "To prove that the hyperplane is a conves state, we consider two point $ x_1 $ and $ x_2 $ on the plane, such that\n",
    "\n",
    "$$ {a^Tx_1 = c} $$\n",
    "$$ {a^Tx_2 = c} $$\n",
    "\n",
    "If this point lies on the hyperplane then the plane is a convex set.\n",
    "$$ \\lambda x_1 + (1 - \\lambda) x_2 \\quad \\forall \\lambda \\in [0, 1] $$\n",
    "\n",
    "Substituting this expression in the $ LHS $ of the hyperplane definition\n",
    "$$ a^T(\\lambda x_1 + (1 - \\lambda) x_2) = a^T \\ \\lambda x_1 + a^T \\ (1 - \\lambda) x_2 = \\lambda \\ a^T x_1 + a^Tx_2 - \\lambda \\ a^Tx_2 = \\lambda \\ c + c - \\lambda \\ c = c = RHS $$\n",
    "\n",
    "Thus we show that the any point on the line joining $ x_1 $ and $ x_2 $ also lies on the plane. Therefore hyperplane is a convex set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859d4a7-f6f0-4263-9262-526a9f4c0cfc",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "The objective function is:\n",
    "\n",
    "$$ \\min_{p} \\quad \\max_{k} \\{h(a^T_k \\ p, I_t)\\} $$\n",
    "\n",
    "Subject to: $ 0 \\le p_i \\le p_{max} $\n",
    "\n",
    "\n",
    "$$\n",
    "I = a^T_kp\n",
    "$$\n",
    "\n",
    "$$\n",
    "h(I, I_t) = \\begin{cases} \n",
    "I_t \\ / \\ I \\quad I \\le I_t \\\\\n",
    "I \\ / \\ I_t \\quad I_t \\le I\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Part (a)\n",
    "\n",
    "To show that the objective function is convex, we start by checking if the function $ h(I, I_t) $ is convex.\n",
    "\n",
    "Gradient of $ h $:\n",
    "\n",
    "$$ \\frac{\\partial h}{\\partial p} =  \\frac{d h}{d I} \\ \\frac{d I}{d p} = h' \\frac{d (a^T_kp)}{d p}  = h'.a$$\n",
    "\n",
    "Hessian of $ h $:\n",
    "\n",
    "$$ \\frac{\\partial^2 h}{\\partial p^2} =  \\frac{d h'}{d I} \\ \\frac{d I}{d p} \\ a^T = h'' \\ a.a^T $$\n",
    "\n",
    "\n",
    "$$\n",
    "h''(I, I_t) = \\begin{cases} \n",
    "2I_t \\ / \\ I^3 \\quad I \\le I_t \\\\\n",
    "1 \\ / \\ I_t \\quad I_t \\le I\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "From the above expression, we can determine that $ h'' \\ge 0 $ since $ I > 0 $.\n",
    "Also, $ a.a^T \\ge 0 $.\n",
    "Therefore, Hessian of $ h $ is positive semi definite. So $ h $ is convex function. \n",
    "\n",
    "Further, the maximum of $ h $ and a scalar $ I_t $ is also a convex function. Therefore the problem is convex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c9474-763d-4f67-8437-e4b8d450643c",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Given function is:\n",
    "\n",
    "$$ c^*(y) = \\max_{x} \\ \\{xy - c(x)\\} $$\n",
    "\n",
    "First check if function $ f(y) = xy - c(x) $ is convex.\n",
    "\n",
    "Gradient of $ f(y) $ is:\n",
    "\n",
    "$$ g(y) = x $$\n",
    "\n",
    "Hessian of $ f(y) $ is:\n",
    "\n",
    "$$ H(y) = 0 $$\n",
    "\n",
    "Since $ f(y) $ is a linear function of $ y $ it follows that it is a convex function. But, maximum function of a convex function is also convex. Hence $ c^*(y) $ is a convex function with respect to $ y $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b163bd-b307-4be3-94be-b98132c55446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
