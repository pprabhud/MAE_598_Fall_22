{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25aeb9c5-b709-45a7-a5a7-53d706b47178",
   "metadata": {},
   "source": [
    "# MAE 598: Design Optimization - Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24039b-678f-440d-ae52-c9f8623285a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "\n",
    "$$ {f(x_1, x_2) = 2x^2_1 - 4x_1x_2 + 1.5x^2_2 + x_2} $$\n",
    "\n",
    "\n",
    "Gradient of $ f(x_1, x_2) $ is:\n",
    "\n",
    "$$ {\n",
    "g(x_1, x_2) = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}\\\\\n",
    "\\frac{\\partial f}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    " = \\begin{bmatrix}\n",
    "4x_1 - 4x_2\\\\\n",
    "-4x_1 + 3x_2 + 1\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "To find the saddle point, $ g(x) = 0 $, solving the the two linear eqauations simultaneously, we get:\n",
    "\n",
    "$$ x_1 = 1, x_2 = 1 $$\n",
    "\n",
    "\n",
    "Hessian of $ f(x_1, x_2) $ is:\n",
    "\n",
    "$$ {\n",
    "H(x_1, x_2) = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x^2_1} & \\frac{\\partial^2 f}{\\partial x_1x_2}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_1x_2} & \\frac{\\partial^2 f}{\\partial x^2_2}\n",
    "\\end{bmatrix}\n",
    " = \\begin{bmatrix}\n",
    "4 & -4\\\\\n",
    "-4 & 3\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "\n",
    "$$ {\n",
    "\\left|\n",
    "\\begin{array}{ccc}\n",
    "H(x_1, x_2) - \\lambda I\n",
    "\\end{array}\n",
    "\\right| = 0\n",
    "} $$\n",
    "\n",
    "$$\n",
    "\\left|\n",
    "\\begin{array}{ccc}\n",
    "4 - \\lambda & -4\\\\\n",
    "-4 & 3 - \\lambda\n",
    "\\end{array}\n",
    "\\right| = (4 - \\lambda) (3 - \\lambda) - 16 = \\lambda^2 - 7\\lambda - 4 = 0\n",
    "$$\n",
    "\n",
    "Solving the characteristic equation, the two Eigenvalues are: $ \\lambda_1 = 7.53, \\lambda_2 = -0.53. $\n",
    "Since the Eigenvalues are both positive and negetive, Hessian is indefinite.\n",
    "\n",
    "\n",
    "\n",
    "Taylor's Second order approximation at the saddle point $x_0 = (1,1)$:\n",
    "\n",
    "$$ {f(x_1, x_2) = f(x_0) +  \\left.g(x)^T\\right|_{x_0} . (x - x_0) + \\frac{1}{2} (x - x_0)^T . \\left.H(x)\\right|_{x_0} . (x - x_0)} $$\n",
    "\n",
    "Evaluating $ f(x_1, x_2) $ and $ g(x_1, x_2) $ at $x_0 = (1,1)$\n",
    "\n",
    "$$ {f(1, 1) = 2 (1)^2 - 4 (1 . 1) + 1.5 (1)^2 + 1 = 0.5} $$\n",
    "\n",
    "$$ {\n",
    "g(1, 1) = \\begin{bmatrix}\n",
    "4 (1) - 4 (1)\\\\\n",
    "-4 (1) + 3 (1) + 1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "Also $\\partial x_i = x_i - 1 $ for $ i = 1, 2 $\n",
    "We can write $ (x - x_0) $ as:\n",
    "\n",
    "$$ {\n",
    "(x - x_0) = \\begin{bmatrix}\n",
    "x_1 - 1\\\\\n",
    "x_2 - 1\n",
    "\\end{bmatrix}\n",
    " = \\begin{bmatrix}\n",
    "\\partial x_1\\\\\n",
    "\\partial x_2\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "\n",
    "Substituting these results back into the approximation function:\n",
    "\n",
    "$$ {f(x_1, x_2) = 0.5 +  \n",
    "\\begin{bmatrix} 0 & 0 \\end{bmatrix} . \n",
    "\\begin{bmatrix}\n",
    "\\partial x_1\\\\\n",
    "\\partial x_2\n",
    "\\end{bmatrix} + \n",
    "\\frac{1}{2} \\begin{bmatrix}\n",
    "\\partial x_1 & \\partial x_2\n",
    "\\end{bmatrix} . \n",
    "\\begin{bmatrix}\n",
    "4 & -4\\\\\n",
    "-4 & 3\n",
    "\\end{bmatrix} . \n",
    "\\begin{bmatrix}\n",
    "\\partial x_1\\\\\n",
    "\\partial x_2\n",
    "\\end{bmatrix}} $$\n",
    "\n",
    "$$ {f(x_1, x_2) = 0.5 +  \n",
    "0 + \n",
    "\\frac{1}{2} \\begin{bmatrix}\n",
    "\\partial x_1 & \\partial x_2\n",
    "\\end{bmatrix} . \n",
    "\\begin{bmatrix}\n",
    "4\\partial x_1 - 4\\partial x_2\\\\\n",
    "-4\\partial x_1 + 3\\partial x_2\n",
    "\\end{bmatrix}\n",
    "} $$\n",
    "\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \\frac{1}{2}\n",
    "(\n",
    "4\\partial^2 x_1 - 4\\partial x_1\\partial x_2 - 4\\partial x_1\\partial x_2 + 3\\partial^2 x_2\n",
    ")\n",
    "} $$\n",
    "\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \\frac{1}{2}\n",
    "(\n",
    "4\\partial^2 x_1 - 8\\partial x_1\\partial x_2 + 3\\partial^2 x_2\n",
    ")\n",
    "} $$\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \\frac{1}{2}\n",
    "(\n",
    "4\\partial^2 x_1 - 8\\partial x_1\\partial x_2 + 3\\partial^2 x_2\n",
    ")\n",
    "} $$\n",
    "\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \\frac{1}{2}\n",
    "(2\\partial x_1 - \\partial x_2) (2\\partial x_1 - 3\\partial x_2)\n",
    "} $$\n",
    "\n",
    "$$ {f(x_1, x_2) =  \n",
    "0.5 + \n",
    "(\\partial x_1 - \\frac{\\partial x_2}{2}) (\\partial x_1 - \\frac{3\\partial x_2}{2})\n",
    "} $$\n",
    "\n",
    "\n",
    "The constants $ a, b, c, d $ are $ 1, -1/2, 1, -3/2 $ respectively.\n",
    "\n",
    "The direction of the downslope is in the negative gradient direction, so we get\n",
    "\n",
    "$$ {f(x_1, x_2) - 0.5 =   \n",
    "(\\partial x_1 - \\frac{\\partial x_2}{2}) . (\\partial x_1 - \\frac{3\\partial x_2}{2}) < 0\n",
    "} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dec9aa-af03-4f6e-96e0-8b7ba9661831",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e50434-50bb-41c4-ba03-e619dafe32f7",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Hyperplane in $ \\mathbb{R}^n $ is defined as:\n",
    "\n",
    "$ {a^Tx = c} $ for $ x \\in \\mathbb{R}^n $, where $ a $ is the direction of the normal to the hyperplane and c is some constant.\n",
    "\n",
    "To prove that the hyperplane is a conves state, we consider two point $ x_1 $ and $ x_2 $ on the plane, such that\n",
    "\n",
    "$$ {a^Tx_1 = c} $$\n",
    "$$ {a^Tx_2 = c} $$\n",
    "\n",
    "If this point lies on the hyperplane then the plane is a convex set.\n",
    "$$ \\lambda x_1 + (1 - \\lambda) x_2 \\quad \\forall \\lambda \\in [0, 1] $$\n",
    "\n",
    "Substituting this expression in the $ LHS $ of the hyperplane definition\n",
    "$$ a^T(\\lambda x_1 + (1 - \\lambda) x_2) = a^T \\ \\lambda x_1 + a^T \\ (1 - \\lambda) x_2 = \\lambda \\ a^T x_1 + a^Tx_2 - \\lambda \\ a^Tx_2 = \\lambda \\ c + c - \\lambda \\ c = c = RHS $$\n",
    "\n",
    "Thus we show that the any point on the line joining $ x_1 $ and $ x_2 $ also lies on the plane. Therefore hyperplane is a convex set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d15aa-b35f-46f4-8580-7b9ca278841e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
