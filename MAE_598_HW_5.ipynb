{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc391ee7-fa2d-4aa4-a7df-e0940989ec89",
   "metadata": {},
   "source": [
    "## SQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd9e4d-2980-4998-9189-598ca3793abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QP\n",
    "\n",
    "# Linesearch with penalty\n",
    "\n",
    "# Active set strategy\n",
    "\n",
    "# BFGS for approximating Hessian\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84531af-e3d1-4123-8fcc-f3eb9692a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd.functional import jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646cb54-ac25-47aa-b520-953a22604305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "\"\"\"\n",
    "Decision variable: x0\n",
    "State variables: x1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "m = 2 # No. of constraints\n",
    "n = 2 # No. of variables\n",
    "d = n - m # No. of decision variables\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    # Define the objective function\n",
    "    f = lambda x: x[0] ** 2 + (x[1] - 3) ** 2\n",
    "    # Define the constraints\n",
    "    g1 = lambda x: (x[1] ** 2) - (2 * x[0])\n",
    "    g2 = lambda x: (x[1] - 1) ** 2 + (5 * x[0]) - 15\n",
    "    \n",
    "    return f(x), h1(x), h2(x)\n",
    "\n",
    "# Reduced Gradient (Analytically computed)\n",
    "# dfdd_analytical = lambda x: ((-5 * x[0] * x[2]) + (-21 * x[0] * x[2]) + (16 * x[1] * x[2])) / (10 * x[1] + 2 * x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff11ac-0d73-42b6-af12-c10a9e84195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Jacobian\n",
    "def jac(x, n=n):\n",
    "    J = t.zeros((n, n))\n",
    "    for i in range(n):\n",
    "       J[i] =  jacobian(f, (x))[i] # 'jacobian' function in Pytorch returns a tuple of tensors. Copying each tensor slice into a new tensor for the ease of indexing.\n",
    "    return J\n",
    "\n",
    "# Evaluate Constraints\n",
    "def hFunc(x, m=m, n=n):\n",
    "    H = t.zeros((m, 1))\n",
    "    for i in range(m):\n",
    "        H[i] =  f(x)[d + i]\n",
    "    return H\n",
    "\n",
    "# Part 3\n",
    "# Calculate the Reduced gradient from the jacobian\n",
    "dfdd = lambda J: J[0,0] - J[0,1:] @ t.pinverse(J[1:,1:]) @ J[1:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01d76f-d5c6-4554-ae74-f639b5af2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levenberg Marquardt solver\n",
    "def LMSolve(x, m=m, n=n, epochs=60, Lambda=1.):      \n",
    "    for i in range(epochs):\n",
    "        H = hFunc(x)\n",
    "        J =  jac(x)\n",
    "        delta = t.pinverse(J[d:, d:].T @ J[d:, d:] + Lambda * t.eye(m)) @ J[d:, d:].T @ H # Change quantity in s_k\n",
    "        with t.no_grad():\n",
    "            x[d:] = x[d:] - delta.T # Update s_k\n",
    "    return x\n",
    "\n",
    "\n",
    "# Update point X\n",
    "def updateX(x, alpha):\n",
    "    xn = t.zeros(3)\n",
    "    J = jac(x)\n",
    "    xn[0] = x[0] - alpha * dfdd(J)\n",
    "    xn[1:] = x[1:] + (alpha * t.pinverse(J[1:,1:]) @ J[1:,0].reshape(2,-1) *  dfdd(J)).T[0]\n",
    "    return xn\n",
    "\n",
    "\n",
    "# Armijo Line search\n",
    "def lineSearch(x, t0=0.5, K=25):\n",
    "    alpha = 1\n",
    "    i = 0\n",
    "    \n",
    "    func = f(updateX(x, alpha))[0]\n",
    "    phi = f(x)[0] - (t0 * alpha * dfdd(jac(x)) ** 2)\n",
    "    \n",
    "    while func > phi and i < K:\n",
    "        alpha = 0.5 * alpha\n",
    "        func = f(updateX(x, alpha))[0]\n",
    "        phi = f(x)[0] - (t0 * alpha * dfdd(jac(x)) ** 2)\n",
    "\n",
    "        i += 1\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ba5c2-8166-47b8-9476-30980c129762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4\n",
    "# Initialization\n",
    "def GRG(x):\n",
    "    # Initialize variables\n",
    "    e = t.norm(dfdd(jac(x)))\n",
    "    tol = 1e-3 # Error threshold\n",
    "\n",
    "    xSol = x.detach().numpy()\n",
    "    fVal = [f(x)[0].item()]\n",
    "    alphaSol = [1]\n",
    "    eVal = [e]\n",
    "\n",
    "    k = 0\n",
    "    while e > tol:\n",
    "        # Part 4.1\n",
    "        # Inexact line search\n",
    "        alpha = lineSearch(x)\n",
    "\n",
    "        # Part 4.2 and 4.3\n",
    "        J = jac(x)\n",
    "        \n",
    "        # Update the point      \n",
    "        with t.no_grad():\n",
    "            x = updateX(x, alpha)\n",
    "        \n",
    "        # Part 4.4\n",
    "        # LM Solver\n",
    "        x = LMSolve(x)\n",
    "\n",
    "        # Part 4.5\n",
    "        e = t.norm(dfdd(jac(x)))\n",
    "        # Store important information in every iteration\n",
    "        xSol = np.vstack((xSol, x.detach().numpy())) # Record x values in each iteration\n",
    "        fVal.append(f(x)[0].item()) # Record f values in each iteration\n",
    "        alphaSol.append(alpha) # Record alpha values in each iteration\n",
    "        eVal.append(e)\n",
    "\n",
    "        k += 1\n",
    "        print (f\"Iteration: {k:<5} Alpha: {alpha:<10} x: {str(x.detach().numpy()) :<40} f(x): {fVal[k]:<20} Error: {e:<20}\")\n",
    "    return xSol, fVal, alphaSol, eVal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9276df-a3ea-48a9-917b-4c478eb1776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# Find the feasible point based on the initial gues of decision variables\n",
    "x = t.tensor([0.1, 1., 1.], dtype=t.float64, requires_grad=True)\n",
    "x = LMSolve(x)\n",
    "print('Feasible starting point: ', x)\n",
    "# print('Reduced gradient:', dfdd(jac(x)))\n",
    "# print('Analytical calculation\\n', dfdd_analytical(x))\n",
    "\n",
    "# Find the optimal solution using GRG\n",
    "print(\"\\nGRG\\n\")\n",
    "xSol, fVal, alphaSol, eVal = GRG(x)\n",
    "print('\\nThe optimal solution is x = ', xSol[-1, :])\n",
    "# Convergence analysis\n",
    "\n",
    "# Plot results\n",
    "print(\"\\nConvergence plot\\n\")\n",
    "plt.semilogy(range(1, len(fVal)+1), eVal, \"rs-\")\n",
    "# plt.xlim(1, 50)\n",
    "plt.xticks(ticks=range(1, len(fVal)+1), labels=range(1, len(fVal)+1))\n",
    "plt.xlabel(\"Iterations (k)\")\n",
    "plt.ylabel(\"Log|Error|\")\n",
    "plt.title(\"Convergence analysis\")\n",
    "plt.grid(axis='y', color='0.95')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae193bcd-5721-421a-b898-96812cf2cdba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f623240-8d52-42d6-abc1-d29586007669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c8d11-7da8-4971-98b5-25e328426c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e953ea7-ab2f-4452-82dc-6d7b57c3ce94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
